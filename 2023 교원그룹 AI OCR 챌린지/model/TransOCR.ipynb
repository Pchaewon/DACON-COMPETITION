{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "FULL_TRAINING = False\n",
    "import random\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pynvml import *\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from transformers import VisionEncoderDecoderModel, AutoTokenizer\n",
    "\n",
    "def print_gpu_utilization():\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n",
    "\n",
    "\n",
    "def print_summary(result):\n",
    "    print(f\"Time: {result.metrics['train_runtime']:.2f}\")\n",
    "    print(f\"Samples/second: {result.metrics['train_samples_per_second']:.2f}\")\n",
    "    print_gpu_utilization()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FULL_TRAINING:\n",
    "    vision_hf_model = 'facebook/deit-base-distilled-patch16-384'\n",
    "    nlp_hf_model = \"klue/roberta-base\"\n",
    "    \n",
    "    # Reference: https://github.com/huggingface/transformers/issues/15823\n",
    "    # initialize the encoder from a pretrained ViT and the decoder from a pretrained BERT model. \n",
    "    # Note that the cross-attention layers will be randomly initialized, and need to be fine-tuned on a downstream dataset\n",
    "    model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(vision_hf_model, nlp_hf_model)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(nlp_hf_model)\n",
    "else:\n",
    "    trocr_model = 'daekeun-ml/ko-trocr-base-nsmc-news-chatbot'\n",
    "    model = VisionEncoderDecoderModel.from_pretrained(trocr_model)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(trocr_model)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  데이터 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'IMG_HEIGHT_SIZE':64,\n",
    "    'IMG_WIDTH_SIZE':224,\n",
    "    'EPOCHS':80,\n",
    "    'LEARNING_RATE':1e-3,\n",
    "    'BATCH_SIZE':256,\n",
    "    'NUM_WORKERS':4, # 본인의 GPU, CPU 환경에 맞게 설정\n",
    "    'SEED':41\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CFG['SEED']) # Seed 고정\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df\u001b[39m=\u001b[39mpd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m'\u001b[39;49m\u001b[39mtrain.csv\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      2\u001b[0m df\u001b[39m.\u001b[39mdrop([\u001b[39m'\u001b[39m\u001b[39mid\u001b[39m\u001b[39m'\u001b[39m],axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m df\u001b[39m.\u001b[39mimg_path\u001b[39m=\u001b[39mdf\u001b[39m.\u001b[39mimg_path\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: x\u001b[39m.\u001b[39mlstrip(\u001b[39m'\u001b[39m\u001b[39m./\u001b[39m\u001b[39m'\u001b[39m))\n",
      "File \u001b[0;32m~/.conda/envs/OCR/lib/python3.10/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/OCR/lib/python3.10/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/OCR/lib/python3.10/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/.conda/envs/OCR/lib/python3.10/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    607\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/.conda/envs/OCR/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m~/.conda/envs/OCR/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[1;32m   1734\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1735\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[1;32m   1736\u001b[0m     f,\n\u001b[1;32m   1737\u001b[0m     mode,\n\u001b[1;32m   1738\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1739\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1740\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m   1741\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[1;32m   1742\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1743\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1744\u001b[0m )\n\u001b[1;32m   1745\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/.conda/envs/OCR/lib/python3.10/site-packages/pandas/io/common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    852\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[1;32m    855\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[1;32m    857\u001b[0m             handle,\n\u001b[1;32m    858\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[1;32m    859\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[1;32m    860\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    861\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    862\u001b[0m         )\n\u001b[1;32m    863\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train.csv'"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv('train.csv')\n",
    "df.drop(['id'],axis=1,inplace=True)\n",
    "df.img_path=df.img_path.apply(lambda x: x.lstrip('./'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['len'] = df['label'].str.len()\n",
    "train_v1 = df[df['len']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = df[df['len']>1]\n",
    "train_v2, val, _, _ = train_test_split(df, df['len'], test_size=0.1, random_state=CFG['SEED'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156157 14718\n"
     ]
    }
   ],
   "source": [
    "train = pd.concat([train_v1, train_v2])\n",
    "print(len(train), len(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=train.drop('len',axis=1)\n",
    "test_df=val.drop('len',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.reset_index(drop=True, inplace=True)\n",
    "test_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 데이터 셋 생성 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OCRDataset(Dataset):\n",
    "    def __init__(self, dataset_dir, df, processor, tokenizer, max_target_length=32):\n",
    "        self.dataset_dir = dataset_dir\n",
    "        self.df = df\n",
    "        self.processor = processor\n",
    "        self.max_target_length = max_target_length\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # get file name + text \n",
    "        file_name = self.df['img_path'][idx]\n",
    "        text = self.df['label'][idx]\n",
    "        # prepare image (i.e. resize + normalize)\n",
    "        image = Image.open(self.dataset_dir + file_name).convert(\"RGB\")\n",
    "        pixel_values = self.processor(image, return_tensors=\"pt\").pixel_values\n",
    "        # add labels (input_ids) by encoding the text      \n",
    "        labels = self.tokenizer(text, padding=\"max_length\", \n",
    "                                stride=32,\n",
    "                                truncation=True,\n",
    "                                max_length=self.max_target_length).input_ids\n",
    "        \n",
    "        # important: make sure that PAD tokens are ignored by the loss function\n",
    "        labels = [label if label != self.tokenizer.pad_token_id else -100 for label in labels]\n",
    "\n",
    "        encoding = {\"pixel_values\": pixel_values.squeeze(), \"labels\": torch.tensor(labels)}\n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 156157\n",
      "Number of validation examples: 14718\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrOCRProcessor\n",
    "processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
    "dataset_dir = './'\n",
    "max_length = 32\n",
    "\n",
    "train_dataset = OCRDataset(\n",
    "    dataset_dir=dataset_dir,\n",
    "    df=train_df,\n",
    "    tokenizer=tokenizer,\n",
    "    processor=processor,\n",
    "    max_target_length=max_length\n",
    ")\n",
    "eval_dataset = OCRDataset(\n",
    "    dataset_dir=dataset_dir,\n",
    "    df=test_df,\n",
    "    tokenizer=tokenizer,\n",
    "    processor=processor,\n",
    "    max_target_length=max_length\n",
    ")\n",
    "\n",
    "print(\"Number of training examples:\", len(train_dataset))\n",
    "print(\"Number of validation examples:\", len(eval_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pixel_values torch.Size([3, 384, 384])\n",
      "labels torch.Size([32])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=90x64>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_idx = np.random.randint(len(train_df))\n",
    "encoding = train_dataset[img_idx]\n",
    "for k,v in encoding.items():\n",
    "    print(k, v.shape)\n",
    "    \n",
    "image = Image.open(train_dataset.dataset_dir + train_df['img_path'][img_idx]).convert(\"RGB\")\n",
    "image    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76888\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    " \n",
    "path_dir = './train'\n",
    " \n",
    "file_list = os.listdir(path_dir)\n",
    "print(len(file_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "의지\n"
     ]
    }
   ],
   "source": [
    "labels = encoding['labels']\n",
    "labels[labels == -100] = tokenizer.pad_token_id\n",
    "label_str = tokenizer.decode(labels, skip_special_tokens=True)\n",
    "print(label_str)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set special tokens used for creating the decoder_input_ids from the labels\n",
    "model.config.decoder_start_token_id = tokenizer.cls_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.vocab_size = model.config.decoder.vocab_size\n",
    "\n",
    "# set beam search parameters\n",
    "model.config.eos_token_id = tokenizer.sep_token_id\n",
    "model.config.max_length = max_length\n",
    "model.config.early_stopping = True\n",
    "model.config.no_repeat_ngram_size = 3\n",
    "model.config.length_penalty = 2.0\n",
    "model.config.num_beams = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    predict_with_generate=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=1,\n",
    "    fp16=True,\n",
    "    learning_rate=4e-5,\n",
    "    output_dir=\"./models\",\n",
    "    save_steps=5000,\n",
    "    eval_steps=5000,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 평가지표 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ebac482fd13421895c344ba999a5b05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.49k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import evaluate\n",
    "cer_metric = evaluate.load(\"cer\")\n",
    "wer_metric = evaluate.load(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "    \n",
    "    pred_str = [pred_str[i] for i in range(len(pred_str)) if len(label_str[i]) > 0]\n",
    "    label_str = [label_str[i] for i in range(len(label_str)) if len(label_str[i]) > 0]\n",
    "\n",
    "    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"cer\": cer, \"wer\": wer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "from transformers import default_data_collator\n",
    "\n",
    "# instantiate trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=default_data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3013"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 156157\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 39040\n",
      "  Number of trainable parameters = 225701120\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './train/TRAIN_21036.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m result \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m~/.conda/envs/OCR/lib/python3.10/site-packages/transformers/trainer.py:1543\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1538\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   1540\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1541\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1542\u001b[0m )\n\u001b[0;32m-> 1543\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1544\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1545\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1546\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1547\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1548\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/OCR/lib/python3.10/site-packages/transformers/trainer.py:1765\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1762\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_load_rng_state(resume_from_checkpoint)\n\u001b[1;32m   1764\u001b[0m step \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m-> 1765\u001b[0m \u001b[39mfor\u001b[39;00m step, inputs \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(epoch_iterator):\n\u001b[1;32m   1766\u001b[0m \n\u001b[1;32m   1767\u001b[0m     \u001b[39m# Skip past any already trained steps if resuming training\u001b[39;00m\n\u001b[1;32m   1768\u001b[0m     \u001b[39mif\u001b[39;00m steps_trained_in_current_epoch \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1769\u001b[0m         steps_trained_in_current_epoch \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/OCR/lib/python3.10/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.conda/envs/OCR/lib/python3.10/site-packages/torch/utils/data/dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.conda/envs/OCR/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.conda/envs/OCR/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[14], line 17\u001b[0m, in \u001b[0;36mOCRDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     15\u001b[0m text \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdf[\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m][idx]\n\u001b[1;32m     16\u001b[0m \u001b[39m# prepare image (i.e. resize + normalize)\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m image \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39;49mopen(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset_dir \u001b[39m+\u001b[39;49m file_name)\u001b[39m.\u001b[39mconvert(\u001b[39m\"\u001b[39m\u001b[39mRGB\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m pixel_values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocessor(image, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mpixel_values\n\u001b[1;32m     19\u001b[0m \u001b[39m# add labels (input_ids) by encoding the text      \u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/OCR/lib/python3.10/site-packages/PIL/Image.py:3131\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3128\u001b[0m     filename \u001b[39m=\u001b[39m fp\n\u001b[1;32m   3130\u001b[0m \u001b[39mif\u001b[39;00m filename:\n\u001b[0;32m-> 3131\u001b[0m     fp \u001b[39m=\u001b[39m builtins\u001b[39m.\u001b[39;49mopen(filename, \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m   3132\u001b[0m     exclusive_fp \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   3134\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './train/TRAIN_21036.png'"
     ]
    }
   ],
   "source": [
    "result = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_test_df = pd.read_csv('/content/data/test.csv')\n",
    "sample_test_df.drop(['id'],axis=1,inplace=True)\n",
    "sample_test_df.img_path=sample_test_df.img_path.apply(lambda x: x.lstrip('./'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장된 모델 불러오기\n",
    "# from transformers import VisionEncoderDecoderModel, AutoTokenizer\n",
    "# model=VisionEncoderDecoderModel.from_pretrained('./model')\n",
    "# tokenizer=AutoTokenizer.from_pretrained('./model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "img_names, outputs= [], []\n",
    "device=torch.device('cuda')\n",
    "model.to(device)\n",
    "for i in tqdm(range(len(sample_test_df))):\n",
    "    image = Image.open(train_dataset.dataset_dir + sample_test_df['img_path'][i]).convert('RGB')\n",
    "    pixel_values =(processor(image,return_tensors='pt').pixel_values).to(device)\n",
    "    generated_ids = model.generate(pixel_values)\n",
    "    generated_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    img_names.append(sample_test_df.img_path[i])\n",
    "    outputs.append(generated_text) \n",
    "    print('Decoded label = {},{}'.format(img_names[i],generated_text))\n",
    "\n",
    "#img_idx = np.random.randint(len(eval_dataaset))\n",
    "#image = Image.open(eval_dataset.dataset_dir + train_df['file_name'][img_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'image_names':img_names, 'outputs':outputs})\n",
    "sub=pd.read_csv('/content/data/sample_submission.csv')\n",
    "sub['label']=df.outputs\n",
    "sub.to_csv('trocr_submit.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OCR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8798f17b1ebb4bf554cdc8d06930db6b85de5ad0efcf63e407b144b979055440"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
